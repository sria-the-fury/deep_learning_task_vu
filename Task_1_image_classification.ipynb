{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Md Jakaria Mashud Shahria (2431751)\n",
        "\n",
        "**Task 1**\n",
        "\n",
        "The first task will require the realization of:\n",
        "\n",
        "*   usage of existing pre-trained (pre-trained) image classification model adaptation to new task using few-shot,one-shot and zero-shot learning.\n",
        "*   calculate accuracy, precision, recovery and F1 statistics for selected new class on unseen 1000 images from OpenImages,\n",
        "*   to implement threshold value (threshold) change, enabling classification of images for each assigned class by changing T∈[0,1]. Statistics must be recalculated after changing the threshold value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "azmkVuurKHIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I tried without gpu, used tensorflow dataset and this method to load dataset:\n",
        "\n",
        "```\n",
        "dataset = tfds.load(‘open_images/v7’, split='train')\n",
        "```\n",
        "\n",
        "Both did not work. Enabling GPU in colab and use FiftyOne package to load openimages_v7 dataset."
      ],
      "metadata": {
        "id": "1149U7wkKhaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhw2ui1sxyGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9b6ca3-0a3e-47a3-9e64-434f188f16b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sse-starlette<1 in /usr/local/lib/python3.12/dist-packages (0.10.3)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.12/dist-packages (from sse-starlette<1) (0.47.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette->sse-starlette<1) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from starlette->sse-starlette<1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette->sse-starlette<1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette->sse-starlette<1) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"sse-starlette<1\"\n",
        "!pip install -q fiftyone transformers datasets scikit-learn tqdm torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use CUDA to get GPU Power, and use OpenAI's ClipModel"
      ],
      "metadata": {
        "id": "YQ9LXE9qLmnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Use a GPU if available (which we enabled in Colab's runtime settings)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# This line securely retrieves the secret you just created\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Load the pre-trained CLIP model and its processor\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449,
          "referenced_widgets": [
            "92149ca7f9cb40db85851f64e7266cff",
            "932cb1a100af4decbc97cbcbef7bbc23",
            "da7fe2b2f0f6433db5fa6ac0b6cca81d",
            "43c10870abf34d01a578511e345697a7",
            "2f26dc408a3c4233a9d5e9b269b826ae",
            "496f73cfabbd43e0b2ac3ab593fbeff7",
            "3764efc170cc47d6ac1b760d14d44704",
            "fcccddb77f534066b47dce1bbfb9bcad",
            "f2a9699c94734d82b3afeb671fca53aa",
            "bf8ffeb2bb674eaca54ac0f2bd0744b0",
            "f366dbe9d67a4ff1ae2d2cca055af88d",
            "140a6fffe28a40029c0500a566c8aba9",
            "d0689c0c6dc346e28aae066288fcfa6f",
            "25691acbe306444f940f4df8c235b82d",
            "68aad456fd074814a05346f42cf93164",
            "efdd1b37862f4a46918107c2ffec0bf8",
            "8ef38b32de2944dda7854bdb15443a45",
            "99f3b874e1b74b56a8009ebbddf7d1be",
            "cbcf1fd0f5154baea3057cb390a79869",
            "0a4fe53fb7a04a608d9d0106d26a56f3",
            "a69c4234f2234e85b513870e39c2fde0",
            "3b0e8cd75e6b4c82b973f912fabefe41",
            "79f9b6d96a1848659e9dec726c69e451",
            "2eb8bae79e4b411cb81daeffcaa492e5",
            "857fbf952cfa4fde8a46c0d87aae6e26",
            "c884533101a940558b87b04ada75d3ee",
            "1e2b3d37b2f04f81afec7beead560172",
            "040f5723a41a40629c25097ceb617402",
            "c46a88f5971e4d91a29a51373f6367fc",
            "a0ff102a64a646e8916d3a35db5c26dd",
            "d94a68ddaf734b658558fa2f71e8ff02",
            "8c62ab6920c64b3eb8b12153114c1aba",
            "dbbf7cec57954e8c97fdb98a388e1c4c",
            "644a0c495e124f38aa084ff7445ae3ec",
            "04d819ace3eb42229d04ad4afdca9160",
            "bfeac1f9e4db4535939eb10834d7f389",
            "26fbbdc4bad24a3884ce748682a5f1c6",
            "9bc2ad96d8e84ad791e6a7b7cfac8b8d",
            "598828ce37154a6cb11d652ba744626d",
            "2ae27b5dae0949f6adb200e77ca24463",
            "abac323334c843559b6c068957e74f5f",
            "93d3567138b9430e884cf4591dcb7da5",
            "fdfc1b7f77fa479cae767d192a45ec06",
            "ccb81a39126f449e93cece3155f17965",
            "c44256710e794510b55d2f6b88af2edb",
            "48fcee7e41514517b0bba5350b2a51b1",
            "4a6ef69f78f242b8a3f7647901d28948",
            "66e3956aecad41cb941793e878ac52a7",
            "ffcdce966e1e4a30a662fee144b19010",
            "e574c1080dde4f4790bf567fd9fa8bdb",
            "e3eddd9f66b6407980d655a42187d3eb",
            "478974ccce5a47d8a51a2e2ba9b19591",
            "284b73074253463993e651f3600f9f3c",
            "19f66273945f45b8bc2d3aec12a26729",
            "c9711b921db94ed79ff918faa48bbc1c",
            "99d1950287f245df8ec4bc61230d2750",
            "733f1777701a4abead219f6757e51a36",
            "b4e18cd25d1b4934ac25346021037336",
            "785b11b73e724e90b631111fd6754d49",
            "3d8a02fee39a411dbd65742f74783e69",
            "e36f6e55105a44ca8b69218f562b8a12",
            "90272bb475674998851b953ed05a24c8",
            "f72176682a374c028c0f3cca2c66c269",
            "dd0f6321bdb549c8828080a223e0edb5",
            "f1c31e51b23843b888182aa6e0098eb1",
            "7f26192d52e54a60a4fde15a09f87656",
            "2be43ba82adb4685b007a52d405cf896",
            "90781a671b454206b791a14f951733f7",
            "865ea420b6e1498f93fe6cdfb930f4d3",
            "64d84acf1164457a914f93ee050fb007",
            "e06d4f12b17f4050ad5b29eae09a0e5f",
            "f1044609c2c3420ca3d6e07bef1d8136",
            "c598f47149f44f4f9ba59e01007ca9b4",
            "54b04339a3864e07bdc2c5b338770c7d",
            "a37cb3cc6fce4be7b03dbe414864622c",
            "a879899dda99426684a805ec50c9eb13",
            "1399b849f7184929ac3bfc6304bbc348",
            "3bd0d683baca484582afb55dbbc0e8ae",
            "a9372c52ea3a4a61a3389812140f5ea6",
            "07195db1d5c6497ebdae97216e7c8dad",
            "889a2398b5974ca48ba751a5aeb559d7",
            "b670cf385739497a8610d0d3bf251a8e",
            "99782497061846de9ae0dcf7ab51f083",
            "e2c340034c6f441aaa2c6c0e98872228",
            "9261657df6d64a81bf4a072295826a4b",
            "3ecdfd6bba7548a983e8e0a1d4bcdb5c",
            "208cec62b0a04f18badb3373329f503c",
            "2fb64193e6014c2799b30a28bdc74524",
            "54a6ce35475a4860a1605f008b0da36e",
            "9d90883dd23a48a396fad05dd130ba2c",
            "3e905df52adf41d397531507fae57b56",
            "541fb82d31f54f5cb94ebd2f8dd5a0bb",
            "9f72c8e5281f4e49bb3791d500eb0176",
            "f8046de71d2e4925937435c36d22fd3a",
            "faf7abaa01674d0ead8d9169763678cb",
            "17d0e1298d524168b5825504856508fe",
            "482c7a6e153b419794dbf12eaec32027",
            "511bcf551ffc449a83785d448f527d34",
            "2bf7bb07abdd444780811c0b63828c03",
            "67b36f3fd48646b48450ab98083bf95a",
            "4821515fcabd45999d4f2387a5c5afda",
            "246c1d7b6ac04c8bb1664d903f3f1d0d",
            "ad34a12c53064b79a6227c3902151a0b",
            "e3f17a45683048359b63579a2f8556e8",
            "ef5e07458d184672baadfa867798d063",
            "f08dc06e96aa49a79bcd4f2c19647fb2",
            "e6fde50fe0e04a0b96d0f71763d5a49b",
            "bc463a7b9694468dad300b140ff37221",
            "cad169001e66492ba6720d9e7ae56fc5",
            "fa9fd032deb14f76a59f8d59a206eb8d"
          ]
        },
        "id": "JM_gOrD3yEXA",
        "outputId": "7b56a9f1-4489-4eb8-eb23-99bf71654cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Successfully logged in to Hugging Face!\n",
            "Loading model: openai/clip-vit-base-patch32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92149ca7f9cb40db85851f64e7266cff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "140a6fffe28a40029c0500a566c8aba9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79f9b6d96a1848659e9dec726c69e451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "644a0c495e124f38aa084ff7445ae3ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c44256710e794510b55d2f6b88af2edb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99d1950287f245df8ec4bc61230d2750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be43ba82adb4685b007a52d405cf896"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bd0d683baca484582afb55dbbc0e8ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54a6ce35475a4860a1605f008b0da36e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67b36f3fd48646b48450ab98083bf95a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPARATION (Using FiftyOne)\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "TARGET_CLASS_NAME = \"Horse\"\n",
        "NUM_EVAL_IMAGES = 1000\n",
        "NUM_POSITIVE_SAMPLES = 500\n",
        "NUM_FEW_SHOT_EXAMPLES = 5  # Number of examples for few-shot learning\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"\n",
        "    Loads and filters the OpenImages v7 dataset using the FiftyOne Zoo.\n",
        "    \"\"\"\n",
        "    print(\"Preparing dataset from the FiftyOne Zoo...\")\n",
        "\n",
        "    # We'll load a larger number of random samples and then filter them.\n",
        "    # This is an easy way to get both positive and negative examples.\n",
        "    num_samples_to_load = NUM_EVAL_IMAGES * 2 # Load more to ensure we find enough of each class\n",
        "\n",
        "    # Load a random subset of the dataset from the zoo\n",
        "    # This downloads only the images and metadata we need.\n",
        "    dataset = foz.load_zoo_dataset(\n",
        "        \"open-images-v7\",\n",
        "        split=\"test\",\n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=num_samples_to_load,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    # Launch the app to visualize the loaded dataset (optional, but very useful!)\n",
        "    # print(\"You can view the loaded dataset in the FiftyOne App:\")\n",
        "    # session = fo.launch_app(dataset, auto=False)\n",
        "    # print(session)\n",
        "\n",
        "    positive_samples, negative_samples, support_samples = [], [], []\n",
        "\n",
        "    print(\"Filtering for positive and negative samples...\")\n",
        "    # Use a view to make processing faster\n",
        "    view = dataset.select_fields(\"ground_truth\")\n",
        "\n",
        "    pbar = tqdm(total=NUM_EVAL_IMAGES + NUM_FEW_SHOT_EXAMPLES)\n",
        "    for sample in view.iter_samples(autosave=True, progress=False):\n",
        "        # Get all labels for the current sample\n",
        "        if not sample.ground_truth:\n",
        "            continue\n",
        "\n",
        "        labels = [d.label for d in sample.ground_truth.detections]\n",
        "\n",
        "        # Load the image from its filepath\n",
        "        pil_image = Image.open(sample.filepath).convert(\"RGB\")\n",
        "\n",
        "        have_enough_support = len(support_samples) >= NUM_FEW_SHOT_EXAMPLES\n",
        "        have_enough_positives = len(positive_samples) >= NUM_POSITIVE_SAMPLES\n",
        "        have_enough_negatives = len(negative_samples) >= (NUM_EVAL_IMAGES - NUM_POSITIVE_SAMPLES)\n",
        "\n",
        "        if TARGET_CLASS_NAME in labels:\n",
        "            if not have_enough_support:\n",
        "                support_samples.append(pil_image)\n",
        "                pbar.update(1)\n",
        "            elif not have_enough_positives:\n",
        "                positive_samples.append(pil_image)\n",
        "                pbar.update(1)\n",
        "        elif not have_enough_negatives:\n",
        "            negative_samples.append(pil_image)\n",
        "            pbar.update(1)\n",
        "\n",
        "        if have_enough_positives and have_enough_negatives and have_enough_support:\n",
        "            break\n",
        "    pbar.close()\n",
        "\n",
        "    # Clean up the downloaded dataset to save space\n",
        "    dataset.delete()\n",
        "\n",
        "    eval_images = positive_samples + negative_samples\n",
        "    true_labels = [1] * len(positive_samples) + [0] * len(negative_samples)\n",
        "\n",
        "    combined = list(zip(eval_images, true_labels))\n",
        "    random.shuffle(combined)\n",
        "    eval_images, true_labels = zip(*combined)\n",
        "\n",
        "    print(f\"\\nDataset prepared: {len(eval_images)} evaluation images and {len(support_samples)} support images.\")\n",
        "    return list(eval_images), list(true_labels), support_samples"
      ],
      "metadata": {
        "id": "fR5kXBW3yKfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION METHODS\n",
        "\n",
        "def predict_zero_shot(image, text_labels):\n",
        "    \"\"\"\n",
        "    Classifies an image using zero-shot learning with text prompts.\n",
        "    Returns the probability score for the first (target) label.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "        outputs = model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "        return probs[0][0].item() # Return probability of the first text label\n",
        "\n",
        "def get_image_embedding(image):\n",
        "    \"\"\"Helper function to get the embedding for a single image.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "        embedding = model.get_image_features(**inputs)\n",
        "        return torch.nn.functional.normalize(embedding, p=2, dim=-1)\n",
        "\n",
        "def predict_few_shot(query_image, support_embeddings):\n",
        "    \"\"\"\n",
        "    Classifies an image by comparing it to the average embedding of support images.\n",
        "    Returns the cosine similarity score.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        query_embedding = get_image_embedding(query_image)\n",
        "        avg_support_embedding = torch.mean(support_embeddings, dim=0, keepdim=True)\n",
        "        similarity = torch.nn.functional.cosine_similarity(query_embedding, avg_support_embedding)\n",
        "        return similarity.item()"
      ],
      "metadata": {
        "id": "A1KMx-rlyQ8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "def calculate_and_print_metrics(scores, true_labels, threshold):\n",
        "    \"\"\"\n",
        "    Calculates and prints classification metrics based on a given threshold.\n",
        "    \"\"\"\n",
        "    predictions = [1 if score >= threshold else 0 for score in scores]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    print(f\"Threshold: {threshold:.2f}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall (Recovery): {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "IZp8n_YvyTuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mani Execution\n",
        "\n",
        "eval_images, true_labels, support_images = prepare_dataset()\n",
        "\n",
        "#ZERO-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Zero-Shot Classification...\")\n",
        "print(\"=\"*50)\n",
        "zero_shot_labels = [f\"a photo of a {TARGET_CLASS_NAME}\", \"a photo of something else\"]\n",
        "zero_shot_scores = [predict_zero_shot(img, zero_shot_labels) for img in tqdm(eval_images, desc=\"Zero-Shot\")]\n",
        "\n",
        "print(\"\\nZero-Shot Evaluation Results:\")\n",
        "for T in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
        "    calculate_and_print_metrics(zero_shot_scores, true_labels, threshold=T)\n",
        "\n",
        "\n",
        "#ONE-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting One-Shot Classification...\")\n",
        "print(\"=\"*50)\n",
        "one_shot_support_embedding = get_image_embedding(support_images[0])\n",
        "one_shot_scores = [predict_few_shot(img, one_shot_support_embedding) for img in tqdm(eval_images, desc=\"One-Shot\")]\n",
        "\n",
        "print(\"\\nOne-Shot Evaluation Results:\")\n",
        "for T in [0.20, 0.25, 0.30, 0.35, 0.40]:\n",
        "    calculate_and_print_metrics(one_shot_scores, true_labels, threshold=T)\n",
        "\n",
        "\n",
        "#FEW-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Starting Few-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Classification...\")\n",
        "print(\"=\"*50)\n",
        "few_shot_support_embeddings = torch.cat([get_image_embedding(img) for img in support_images], dim=0)\n",
        "few_shot_scores = [predict_few_shot(img, few_shot_support_embeddings) for img in tqdm(eval_images, desc=\"Few-Shot\")]\n",
        "\n",
        "print(f\"\\nFew-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Evaluation Results:\")\n",
        "for T in [0.20, 0.25, 0.30, 0.35, 0.40]:\n",
        "    calculate_and_print_metrics(few_shot_scores, true_labels, threshold=T)"
      ],
      "metadata": {
        "id": "qiw7CLciyj0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b49c55-3f52-4799-8991-77ee7aecdffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset from the FiftyOne Zoo...\n",
            "Downloading split 'test' to '/root/fiftyone/open-images-v7/test' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'test' to '/root/fiftyone/open-images-v7/test' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to '/root/fiftyone/open-images-v7/test/metadata/image_ids.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to '/root/fiftyone/open-images-v7/test/metadata/image_ids.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v7/test/metadata/classes.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v7/test/metadata/classes.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpesmpju7f/metadata/hierarchy.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpesmpju7f/metadata/hierarchy.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to '/root/fiftyone/open-images-v7/test/labels/detections.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to '/root/fiftyone/open-images-v7/test/labels/detections.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 2000 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 2000 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 2000/2000 [4.1m elapsed, 0s remaining, 6.7 files/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 2000/2000 [4.1m elapsed, 0s remaining, 6.7 files/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'open-images-v7' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 2000/2000 [15.6s elapsed, 0s remaining, 236.7 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 2000/2000 [15.6s elapsed, 0s remaining, 236.7 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'open-images-v7-test-2000' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'open-images-v7-test-2000' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering for positive and negative samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 509/1005 [00:22<00:21, 22.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset prepared: 504 evaluation images and 5 support images.\n",
            "\n",
            "==================================================\n",
            "Starting Zero-Shot Classification...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Zero-Shot: 100%|██████████| 504/504 [00:15<00:00, 33.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-Shot Evaluation Results:\n",
            "Threshold: 0.10\n",
            "  Accuracy:  0.6052\n",
            "  Precision: 0.0197\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0386\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.8571\n",
            "  Precision: 0.0526\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1000\n",
            "------------------------------\n",
            "Threshold: 0.50\n",
            "  Accuracy:  0.9246\n",
            "  Precision: 0.0952\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1739\n",
            "------------------------------\n",
            "Threshold: 0.70\n",
            "  Accuracy:  0.9683\n",
            "  Precision: 0.2000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3333\n",
            "------------------------------\n",
            "Threshold: 0.90\n",
            "  Accuracy:  0.9881\n",
            "  Precision: 0.4000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.5714\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "Starting One-Shot Classification...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "One-Shot: 100%|██████████| 504/504 [00:11<00:00, 44.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-Shot Evaluation Results:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0099\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0158\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0099\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0158\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0159\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0159\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0397\n",
            "  Precision: 0.0082\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0163\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.1071\n",
            "  Precision: 0.0088\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0175\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "Starting Few-Shot (5 examples) Classification...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-Shot: 100%|██████████| 504/504 [00:11<00:00, 44.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few-Shot (5 examples) Evaluation Results:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0099\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0158\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0099\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0158\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0119\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0158\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0198\n",
            "  Precision: 0.0080\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0159\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.0377\n",
            "  Precision: 0.0082\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0162\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}
