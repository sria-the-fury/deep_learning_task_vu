{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPssKrKBa9E1rb8X/Ek/b6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e945eefad8d42a6913c81cbca3afcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83ff08ccd8d5421b814082f78df33062",
              "IPY_MODEL_0fa0cb74132646dba65476f6102f019c",
              "IPY_MODEL_fe67815bd06643f7aae10b67d47f93b7"
            ],
            "layout": "IPY_MODEL_6f9c741b0db341f28617ad78eb74b168"
          }
        },
        "83ff08ccd8d5421b814082f78df33062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a596171375274626a06c2502edd56e6e",
            "placeholder": "​",
            "style": "IPY_MODEL_a89956f8ebde4911940f5b851d001f2f",
            "value": "Fetching 1 files: 100%"
          }
        },
        "0fa0cb74132646dba65476f6102f019c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7fe4c98b3144c8f818135f04f0b379c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4807ef4dd95d45b8b4679d2e36faf5c3",
            "value": 1
          }
        },
        "fe67815bd06643f7aae10b67d47f93b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6c7a4558c949a893a51556fcefa972",
            "placeholder": "​",
            "style": "IPY_MODEL_4e42c056050546b088136dcdb8fb18b1",
            "value": " 1/1 [00:00&lt;00:00, 28.09it/s]"
          }
        },
        "6f9c741b0db341f28617ad78eb74b168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a596171375274626a06c2502edd56e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89956f8ebde4911940f5b851d001f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7fe4c98b3144c8f818135f04f0b379c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4807ef4dd95d45b8b4679d2e36faf5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e6c7a4558c949a893a51556fcefa972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e42c056050546b088136dcdb8fb18b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Md Jakaria Mashud Shahria (2431751)\n",
        "\n",
        "**Task 1**\n",
        "\n",
        "The first task will require the realization of:\n",
        "\n",
        "*   usage of existing pre-trained (pre-trained) image classification model adaptation to new task using few-shot,one-shot and zero-shot learning.\n",
        "*   calculate accuracy, precision, recovery and F1 statistics for selected new class on unseen 1000 images from OpenImages,\n",
        "*   to implement threshold value (threshold) change, enabling classification of images for each assigned class by changing T∈[0,1]. Statistics must be recalculated after changing the threshold value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "azmkVuurKHIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I tried without gpu, used tensorflow dataset and this method to load dataset:\n",
        "\n",
        "```\n",
        "dataset = tfds.load(‘open_images/v7’, split='train')\n",
        "```\n",
        "\n",
        "Both did not work. Enabling GPU in colab and use FiftyOne package to load openimages_v7 dataset."
      ],
      "metadata": {
        "id": "1149U7wkKhaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xhw2ui1sxyGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab437759-5c85-42c3-906f-0528937adee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sse-starlette<1 in /usr/local/lib/python3.12/dist-packages (0.10.3)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.12/dist-packages (from sse-starlette<1) (0.47.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette->sse-starlette<1) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from starlette->sse-starlette<1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette->sse-starlette<1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette->sse-starlette<1) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"sse-starlette<1\"\n",
        "!pip install -q fiftyone transformers datasets scikit-learn tqdm torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use CUDA to get GPU Power, and use OpenAI's ClipModel"
      ],
      "metadata": {
        "id": "YQ9LXE9qLmnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Use a GPU if available (which we enabled in Colab's runtime settings)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# This line securely retrieves the secret you just created\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Load the pre-trained CLIP model and its processor\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "8e945eefad8d42a6913c81cbca3afcd6",
            "83ff08ccd8d5421b814082f78df33062",
            "0fa0cb74132646dba65476f6102f019c",
            "fe67815bd06643f7aae10b67d47f93b7",
            "6f9c741b0db341f28617ad78eb74b168",
            "a596171375274626a06c2502edd56e6e",
            "a89956f8ebde4911940f5b851d001f2f",
            "f7fe4c98b3144c8f818135f04f0b379c",
            "4807ef4dd95d45b8b4679d2e36faf5c3",
            "5e6c7a4558c949a893a51556fcefa972",
            "4e42c056050546b088136dcdb8fb18b1"
          ]
        },
        "id": "JM_gOrD3yEXA",
        "outputId": "f36fa0ef-ca6a-4caf-8ca0-a0adf15f7d1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Successfully logged in to Hugging Face!\n",
            "Loading model: openai/clip-vit-base-patch32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e945eefad8d42a6913c81cbca3afcd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPARATION (Using FiftyOne)\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "TARGET_CLASSES = [\"Horse\", \"Cat\", \"Dog\"]  # Changed to a list of target classes\n",
        "NUM_EVAL_IMAGES_PER_CLASS = 50 # Reduced number of evaluation images per class (positive)\n",
        "NUM_NEGATIVE_IMAGES_PER_CLASS = 20 # Reduced number of negative images per class\n",
        "NUM_FEW_SHOT_EXAMPLES = 3  # Reduced number of examples for few-shot learning per class\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"\n",
        "    Loads and filters the OpenImages v7 dataset using the FiftyOne Zoo.\n",
        "    Collects positive and negative samples for multiple target classes.\n",
        "    \"\"\"\n",
        "    print(\"Preparing dataset from the FiftyOne Zoo...\")\n",
        "\n",
        "    # We'll load a larger number of random samples and then filter them.\n",
        "    # This is an easy way to get both positive and negative examples.\n",
        "    num_samples_to_load = (NUM_EVAL_IMAGES_PER_CLASS + NUM_NEGATIVE_IMAGES_PER_CLASS + NUM_FEW_SHOT_EXAMPLES) * len(TARGET_CLASSES) * 2 # Load more to ensure we find enough of each class\n",
        "\n",
        "    # Load a random subset of the dataset from the zoo\n",
        "    # This downloads only the images and metadata we need.\n",
        "    dataset = foz.load_zoo_dataset(\n",
        "        \"open-images-v7\",\n",
        "        split=\"test\",\n",
        "        label_types=[\"detections\"],\n",
        "        max_samples=num_samples_to_load,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    positive_samples_by_class = {cls: [] for cls in TARGET_CLASSES}\n",
        "    negative_samples = []\n",
        "    support_samples_by_class = {cls: [] for cls in TARGET_CLASSES}\n",
        "\n",
        "    print(\"Filtering for positive and negative samples...\")\n",
        "    # Use a view to make processing faster\n",
        "    view = dataset.select_fields(\"ground_truth\")\n",
        "\n",
        "    total_samples_needed = (NUM_EVAL_IMAGES_PER_CLASS * len(TARGET_CLASSES)) + (NUM_NEGATIVE_IMAGES_PER_CLASS * len(TARGET_CLASSES)) + (NUM_FEW_SHOT_EXAMPLES * len(TARGET_CLASSES))\n",
        "    pbar = tqdm(total=total_samples_needed)\n",
        "\n",
        "    for sample in view.iter_samples(autosave=True, progress=False):\n",
        "        # Get all labels for the current sample\n",
        "        if not sample.ground_truth:\n",
        "            continue\n",
        "\n",
        "        labels = [d.label for d in sample.ground_truth.detections]\n",
        "\n",
        "        # Load the image from its filepath\n",
        "        try:\n",
        "            pil_image = Image.open(sample.filepath).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            # Skip images that cannot be opened\n",
        "            # print(f\"Could not open image {sample.filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Check for support samples\n",
        "        for target_class in TARGET_CLASSES:\n",
        "            if target_class in labels and len(support_samples_by_class[target_class]) < NUM_FEW_SHOT_EXAMPLES:\n",
        "                support_samples_by_class[target_class].append(pil_image)\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Check for positive and negative evaluation samples\n",
        "        is_target_class = any(cls in labels for cls in TARGET_CLASSES)\n",
        "\n",
        "        if is_target_class:\n",
        "            for target_class in TARGET_CLASSES:\n",
        "                if target_class in labels and len(positive_samples_by_class[target_class]) < NUM_EVAL_IMAGES_PER_CLASS:\n",
        "                     positive_samples_by_class[target_class].append(pil_image)\n",
        "                     pbar.update(1)\n",
        "        elif len(negative_samples) < NUM_NEGATIVE_IMAGES_PER_CLASS * len(TARGET_CLASSES): # Collect negative samples for all classes\n",
        "             negative_samples.append(pil_image)\n",
        "             pbar.update(1)\n",
        "\n",
        "\n",
        "        # Check if we have enough samples of all types\n",
        "        have_enough_support = all(len(support_samples_by_class[cls]) >= NUM_FEW_SHOT_EXAMPLES for cls in TARGET_CLASSES)\n",
        "        have_enough_positives = all(len(positive_samples_by_class[cls]) >= NUM_EVAL_IMAGES_PER_CLASS for cls in TARGET_CLASSES)\n",
        "        have_enough_negatives = len(negative_samples) >= NUM_NEGATIVE_IMAGES_PER_CLASS * len(TARGET_CLASSES)\n",
        "\n",
        "\n",
        "        if have_enough_positives and have_enough_negatives and have_enough_support:\n",
        "            break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # Clean up the downloaded dataset to save space\n",
        "    dataset.delete()\n",
        "\n",
        "    eval_images = []\n",
        "    true_labels = []\n",
        "    support_images = {cls: support_samples_by_class[cls] for cls in TARGET_CLASSES}\n",
        "\n",
        "    # Combine positive and negative samples for evaluation\n",
        "    for i, target_class in enumerate(TARGET_CLASSES):\n",
        "        eval_images.extend(positive_samples_by_class[target_class])\n",
        "        true_labels.extend([target_class] * len(positive_samples_by_class[target_class]))\n",
        "\n",
        "    eval_images.extend(negative_samples)\n",
        "    true_labels.extend([\"Negative\"] * len(negative_samples)) # Assign a generic \"Negative\" label\n",
        "\n",
        "\n",
        "    combined = list(zip(eval_images, true_labels))\n",
        "    random.shuffle(combined)\n",
        "    eval_images, true_labels = zip(*combined)\n",
        "\n",
        "    print(f\"\\nDataset prepared: {len(eval_images)} evaluation images and {NUM_FEW_SHOT_EXAMPLES} support images per class.\")\n",
        "    return list(eval_images), list(true_labels), support_images"
      ],
      "metadata": {
        "id": "fR5kXBW3yKfq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION METHODS\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "def predict_zero_shot(image, text_labels):\n",
        "    \"\"\"\n",
        "    Classifies an image using zero-shot learning with text prompts.\n",
        "    Returns the probability score for each provided text label.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "        outputs = model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "        return probs[0].tolist() # Return probabilities for all text labels\n",
        "\n",
        "def get_image_embedding(image):\n",
        "    \"\"\"Helper function to get the embedding for a single image.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "        embedding = model.get_image_features(**inputs)\n",
        "        return torch.nn.functional.normalize(embedding, p=2, dim=-1)\n",
        "\n",
        "def predict_few_shot(query_image, support_embeddings_by_class):\n",
        "    \"\"\"\n",
        "    Classifies an image by comparing it to the average embedding of support images for each class.\n",
        "    Returns the cosine similarity score for each class.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        query_embedding = get_image_embedding(query_image)\n",
        "        similarities = {}\n",
        "        for class_name, support_embeddings in support_embeddings_by_class.items():\n",
        "            if support_embeddings.numel() > 0:  # Check if tensor is not empty\n",
        "                # Ensure avg_support_embedding is a single vector\n",
        "                avg_support_embedding = torch.mean(support_embeddings, dim=0, keepdim=True)\n",
        "\n",
        "                # Calculate cosine similarity manually\n",
        "                dot_product = torch.sum(query_embedding * avg_support_embedding, dim=-1)\n",
        "                # Since embeddings are already L2 normalized, the dot product is the cosine similarity\n",
        "                similarity = dot_product\n",
        "\n",
        "                # Get the scalar value\n",
        "                similarities[class_name] = similarity.item()\n",
        "            else:\n",
        "                 similarities[class_name] = 0.0  # Assign 0.0 if no support embeddings\n",
        "        return similarities"
      ],
      "metadata": {
        "id": "A1KMx-rlyQ8D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "\n",
        "def calculate_and_print_metrics(scores, true_labels, threshold):\n",
        "    \"\"\"\n",
        "    Calculates and prints classification metrics based on a given threshold.\n",
        "    \"\"\"\n",
        "    predictions = [1 if score >= threshold else 0 for score in scores]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
        "\n",
        "    print(f\"Threshold: {threshold:.2f}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall (Recovery): {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "IZp8n_YvyTuL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mani Execution\n",
        "\n",
        "eval_images, true_labels, support_images = prepare_dataset()\n",
        "\n",
        "#ZERO-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Zero-Shot Classification...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for target_class in TARGET_CLASSES:\n",
        "    print(f\"\\nEvaluating Zero-Shot for class: {target_class}\")\n",
        "    zero_shot_labels = [f\"a photo of a {target_class}\", \"a photo of something else\"]\n",
        "    # Filter eval_images and true_labels for the current target class and negative samples\n",
        "    class_eval_images = [img for img, label in zip(eval_images, true_labels) if label == target_class or label == \"Negative\"]\n",
        "    class_true_labels = [1 if label == target_class else 0 for label in true_labels if label == target_class or label == \"Negative\"]\n",
        "\n",
        "    # Extract the probability of the target class\n",
        "    zero_shot_scores = [predict_zero_shot(img, zero_shot_labels)[0] for img in tqdm(class_eval_images, desc=f\"Zero-Shot ({target_class})\")]\n",
        "\n",
        "    print(f\"\\nZero-Shot Evaluation Results for {target_class}:\")\n",
        "\n",
        "    for T in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
        "        calculate_and_print_metrics(zero_shot_scores, class_true_labels, threshold=T)\n",
        "\n",
        "\n",
        "#ONE-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting One-Shot Classification...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for target_class in TARGET_CLASSES:\n",
        "    print(f\"\\nEvaluating One-Shot for class: {target_class}\")\n",
        "    if support_images[target_class]:\n",
        "        one_shot_support_embedding = get_image_embedding(support_images[target_class][0])\n",
        "\n",
        "        # Filter eval_images and true_labels for the current target class and negative samples\n",
        "        class_eval_images = [img for img, label in zip(eval_images, true_labels) if label == target_class or label == \"Negative\"]\n",
        "        class_true_labels = [1 if label == target_class else 0 for label in true_labels if label == target_class or label == \"Negative\"]\n",
        "\n",
        "        # Extract the similarity score for the target class\n",
        "        one_shot_scores = [predict_few_shot(img, {target_class: one_shot_support_embedding.unsqueeze(0)})[target_class] for img in tqdm(class_eval_images, desc=f\"One-Shot ({target_class})\")]\n",
        "\n",
        "\n",
        "        print(f\"\\nOne-Shot Evaluation Results for {target_class}:\")\n",
        "\n",
        "        for T in [0.20, 0.25, 0.30, 0.35, 0.40]:\n",
        "            calculate_and_print_metrics(one_shot_scores, class_true_labels, threshold=T)\n",
        "    else:\n",
        "        print(f\"  No support images found for {target_class}. Skipping One-Shot evaluation.\")\n",
        "\n",
        "\n",
        "#FEW-SHOT LEARNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Starting Few-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Classification...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for target_class in TARGET_CLASSES:\n",
        "    print(f\"\\nEvaluating Few-Shot for class: {target_class}\")\n",
        "    if support_images[target_class]:\n",
        "        few_shot_support_embeddings = torch.cat([get_image_embedding(img) for img in support_images[target_class]], dim=0)\n",
        "\n",
        "        # Filter eval_images and true_labels for the current target class and negative samples\n",
        "        class_eval_images = [img for img, label in zip(eval_images, true_labels) if label == target_class or label == \"Negative\"]\n",
        "        class_true_labels = [1 if label == target_class else 0 for label in true_labels if label == target_class or label == \"Negative\"]\n",
        "\n",
        "        # Extract the similarity score for the target class\n",
        "        few_shot_scores = [predict_few_shot(img, {target_class: few_shot_support_embeddings})[target_class] for img in tqdm(class_eval_images, desc=f\"Few-Shot ({target_class})\")]\n",
        "\n",
        "\n",
        "        print(f\"\\nFew-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Evaluation Results for {target_class}:\")\n",
        "\n",
        "        for T in [0.20, 0.25, 0.30, 0.35, 0.40]:\n",
        "            calculate_and_print_metrics(few_shot_scores, class_true_labels, threshold=T)\n",
        "    else:\n",
        "        print(f\"  No support images found for {target_class}. Skipping Few-Shot evaluation.\")"
      ],
      "metadata": {
        "id": "qiw7CLciyj0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42cbe817-1d66-41cd-c3e8-e1c186d73b9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset from the FiftyOne Zoo...\n",
            "Downloading split 'test' to '/root/fiftyone/open-images-v7/test' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'test' to '/root/fiftyone/open-images-v7/test' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.openimages:Necessary images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'test' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'test' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'open-images-v7' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 438/438 [7.3s elapsed, 0s remaining, 72.6 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 438/438 [7.3s elapsed, 0s remaining, 72.6 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'open-images-v7-test-438' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'open-images-v7-test-438' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering for positive and negative samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 42%|████▏     | 93/219 [00:10<00:13,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset prepared: 85 evaluation images and 3 support images per class.\n",
            "\n",
            "==================================================\n",
            "Starting Zero-Shot Classification...\n",
            "==================================================\n",
            "\n",
            "Evaluating Zero-Shot for class: Horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Zero-Shot (Horse): 100%|██████████| 65/65 [00:03<00:00, 20.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-Shot Evaluation Results for Horse:\n",
            "Threshold: 0.10\n",
            "  Accuracy:  0.4769\n",
            "  Precision: 0.1282\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.2273\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.8308\n",
            "  Precision: 0.3125\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.4762\n",
            "------------------------------\n",
            "Threshold: 0.50\n",
            "  Accuracy:  0.9231\n",
            "  Precision: 0.5000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.6667\n",
            "------------------------------\n",
            "Threshold: 0.70\n",
            "  Accuracy:  0.9692\n",
            "  Precision: 0.7143\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.8333\n",
            "------------------------------\n",
            "Threshold: 0.90\n",
            "  Accuracy:  0.9846\n",
            "  Precision: 0.8333\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.9091\n",
            "------------------------------\n",
            "\n",
            "Evaluating Zero-Shot for class: Cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Zero-Shot (Cat): 100%|██████████| 62/62 [00:06<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-Shot Evaluation Results for Cat:\n",
            "Threshold: 0.10\n",
            "  Accuracy:  0.9032\n",
            "  Precision: 0.2500\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.4000\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.9677\n",
            "  Precision: 0.5000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.6667\n",
            "------------------------------\n",
            "Threshold: 0.50\n",
            "  Accuracy:  0.9839\n",
            "  Precision: 0.6667\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.8000\n",
            "------------------------------\n",
            "Threshold: 0.70\n",
            "  Accuracy:  1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  1.0000\n",
            "------------------------------\n",
            "Threshold: 0.90\n",
            "  Accuracy:  1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  1.0000\n",
            "------------------------------\n",
            "\n",
            "Evaluating Zero-Shot for class: Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Zero-Shot (Dog): 100%|██████████| 78/78 [00:04<00:00, 18.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-Shot Evaluation Results for Dog:\n",
            "Threshold: 0.10\n",
            "  Accuracy:  0.6154\n",
            "  Precision: 0.3750\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.5455\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.9231\n",
            "  Precision: 0.7500\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.8571\n",
            "------------------------------\n",
            "Threshold: 0.50\n",
            "  Accuracy:  0.9872\n",
            "  Precision: 0.9474\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.9730\n",
            "------------------------------\n",
            "Threshold: 0.70\n",
            "  Accuracy:  0.9872\n",
            "  Precision: 0.9474\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.9730\n",
            "------------------------------\n",
            "Threshold: 0.90\n",
            "  Accuracy:  0.9872\n",
            "  Precision: 1.0000\n",
            "  Recall (Recovery): 0.9444\n",
            "  F1-Score:  0.9714\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "Starting One-Shot Classification...\n",
            "==================================================\n",
            "\n",
            "Evaluating One-Shot for class: Horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "One-Shot (Horse): 100%|██████████| 65/65 [00:02<00:00, 25.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-Shot Evaluation Results for Horse:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.1077\n",
            "  Precision: 0.0794\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1471\n",
            "------------------------------\n",
            "\n",
            "Evaluating One-Shot for class: Cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "One-Shot (Cat): 100%|██████████| 62/62 [00:03<00:00, 15.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-Shot Evaluation Results for Cat:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0484\n",
            "  Precision: 0.0328\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0635\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.1129\n",
            "  Precision: 0.0351\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0678\n",
            "------------------------------\n",
            "\n",
            "Evaluating One-Shot for class: Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "One-Shot (Dog): 100%|██████████| 78/78 [00:05<00:00, 14.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-Shot Evaluation Results for Dog:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.2564\n",
            "  Precision: 0.2368\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3830\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.3077\n",
            "  Precision: 0.2500\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.4000\n",
            "------------------------------\n",
            "\n",
            "==================================================\n",
            "Starting Few-Shot (3 examples) Classification...\n",
            "==================================================\n",
            "\n",
            "Evaluating Few-Shot for class: Horse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-Shot (Horse): 100%|██████████| 65/65 [00:01<00:00, 39.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few-Shot (3 examples) Evaluation Results for Horse:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0769\n",
            "  Precision: 0.0769\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1429\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0923\n",
            "  Precision: 0.0781\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1449\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.2000\n",
            "  Precision: 0.0877\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.1613\n",
            "------------------------------\n",
            "\n",
            "Evaluating Few-Shot for class: Cat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-Shot (Cat): 100%|██████████| 62/62 [00:01<00:00, 50.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few-Shot (3 examples) Evaluation Results for Cat:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.0323\n",
            "  Precision: 0.0323\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0625\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.0484\n",
            "  Precision: 0.0328\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0635\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.1613\n",
            "  Precision: 0.0370\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.0714\n",
            "------------------------------\n",
            "\n",
            "Evaluating Few-Shot for class: Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-Shot (Dog): 100%|██████████| 78/78 [00:01<00:00, 50.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few-Shot (3 examples) Evaluation Results for Dog:\n",
            "Threshold: 0.20\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.25\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.30\n",
            "  Accuracy:  0.2308\n",
            "  Precision: 0.2308\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3750\n",
            "------------------------------\n",
            "Threshold: 0.35\n",
            "  Accuracy:  0.2564\n",
            "  Precision: 0.2368\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.3830\n",
            "------------------------------\n",
            "Threshold: 0.40\n",
            "  Accuracy:  0.3462\n",
            "  Precision: 0.2609\n",
            "  Recall (Recovery): 1.0000\n",
            "  F1-Score:  0.4138\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57881dc9",
        "outputId": "c167679e-8e92-44b9-973b-23d7fb85abc0"
      },
      "source": [
        "# Choose sample images from the evaluation set\n",
        "if eval_images:\n",
        "    num_demonstration_images = 5 # Number of images to demonstrate\n",
        "\n",
        "    # Filter for images that are one of the target classes for demonstration\n",
        "    target_class_eval_images = [img for img, label in zip(eval_images, true_labels) if label in TARGET_CLASSES]\n",
        "    target_class_true_labels = [label for label in true_labels if label in TARGET_CLASSES]\n",
        "\n",
        "    if not target_class_eval_images:\n",
        "        print(\"No images from target classes available for demonstration.\")\n",
        "    else:\n",
        "        demonstration_true_labels = target_class_true_labels[:min(num_demonstration_images, len(target_class_true_labels))]\n",
        "        zero_shot_predictions = []\n",
        "        one_shot_predictions = []\n",
        "        few_shot_predictions = []\n",
        "\n",
        "        for i in range(min(num_demonstration_images, len(target_class_eval_images))):\n",
        "            sample_image = target_class_eval_images[i]\n",
        "            sample_true_label = target_class_true_labels[i]\n",
        "            print(f\"\\n--- Demonstration Image {i+1} ---\")\n",
        "            print(f\"Sample True Label: {sample_true_label}\")\n",
        "\n",
        "\n",
        "            # --- Zero-Shot Probabilities ---\n",
        "            print(\"\\nZero-Shot Probabilities:\")\n",
        "            # Create labels for all target classes\n",
        "            zero_shot_labels = [f\"a photo of a {cls}\" for cls in TARGET_CLASSES]\n",
        "            zero_shot_probs = predict_zero_shot(sample_image, zero_shot_labels)\n",
        "\n",
        "            # Print probabilities for each target class\n",
        "            for j, class_name in enumerate(TARGET_CLASSES):\n",
        "                print(f\"  {class_name}: {zero_shot_probs[j]:.4f}\")\n",
        "\n",
        "            # Zero-shot prediction\n",
        "            predicted_index = zero_shot_probs.index(max(zero_shot_probs))\n",
        "            predicted_label_zero_shot = zero_shot_labels[predicted_index].replace(\"a photo of a \", \"\")\n",
        "            print(f\"Predicted (Zero-Shot): {predicted_label_zero_shot}\")\n",
        "            zero_shot_predictions.append(predicted_label_zero_shot)\n",
        "\n",
        "\n",
        "            # --- One-Shot Probabilities ---\n",
        "            print(\"\\nOne-Shot Probabilities:\")\n",
        "            one_shot_similarities = {}\n",
        "            one_shot_support_embeddings = {}\n",
        "            for target_class in TARGET_CLASSES:\n",
        "                if support_images[target_class]:\n",
        "                    one_shot_support_embeddings[target_class] = get_image_embedding(support_images[target_class][0]).unsqueeze(0) # Keep embeddings as [1, 512] tensors\n",
        "                else:\n",
        "                     one_shot_support_embeddings[target_class] = torch.tensor([]).to(DEVICE) # Handle case with no support images\n",
        "\n",
        "            one_shot_similarities = predict_few_shot(sample_image, one_shot_support_embeddings)\n",
        "\n",
        "            # Print similarity scores for each target class\n",
        "            for class_name in TARGET_CLASSES:\n",
        "                 print(f\"  {class_name}: {one_shot_similarities[class_name]:.4f}\")\n",
        "\n",
        "            # One-shot prediction (based on highest similarity)\n",
        "            predicted_label_one_shot = max(one_shot_similarities, key=one_shot_similarities.get)\n",
        "            print(f\"Predicted (One-Shot): {predicted_label_one_shot}\")\n",
        "            one_shot_predictions.append(predicted_label_one_shot)\n",
        "\n",
        "\n",
        "            # --- Few-Shot Probabilities ---\n",
        "            print(f\"\\nFew-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Probabilities):\")\n",
        "            few_shot_similarities = {}\n",
        "            support_embeddings_for_few_shot = {}\n",
        "            for target_class in TARGET_CLASSES:\n",
        "                if support_images[target_class]:\n",
        "                    support_embeddings_for_few_shot[target_class] = torch.cat([get_image_embedding(img) for img in support_images[target_class]], dim=0)\n",
        "                else:\n",
        "                     support_embeddings_for_few_shot[target_class] = torch.tensor([]).to(DEVICE) # Handle case with no support images\n",
        "\n",
        "\n",
        "            few_shot_similarities = predict_few_shot(sample_image, support_embeddings_for_few_shot)\n",
        "\n",
        "            # Print similarity scores for each target class\n",
        "            for class_name in TARGET_CLASSES:\n",
        "                 print(f\"  {class_name}: {few_shot_similarities[class_name]:.4f}\")\n",
        "\n",
        "            # Few-shot prediction (based on highest similarity)\n",
        "            predicted_label_few_shot = max(few_shot_similarities, key=few_shot_similarities.get)\n",
        "            print(f\"Predicted (Few-Shot): {predicted_label_few_shot}\")\n",
        "            few_shot_predictions.append(predicted_label_few_shot)\n",
        "\n",
        "\n",
        "          # Calculate and print overall accuracies\n",
        "        overall_zero_shot_accuracy = accuracy_score(demonstration_true_labels, zero_shot_predictions)\n",
        "        overall_one_shot_accuracy = accuracy_score(demonstration_true_labels, one_shot_predictions)\n",
        "        overall_few_shot_accuracy = accuracy_score(demonstration_true_labels, few_shot_predictions)\n",
        "\n",
        "        print(f\"\\n--- Overall Accuracy on Demonstration {num_demonstration_images} Images ---\")\n",
        "        print(f\"Zero-Shot Accuracy: {overall_zero_shot_accuracy:.4f}\")\n",
        "        print(f\"One-Shot Accuracy: {overall_one_shot_accuracy:.4f}\")\n",
        "        print(f\"Few-Shot ({NUM_FEW_SHOT_EXAMPLES} examples) Accuracy: {overall_few_shot_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"No evaluation images available to demonstrate.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Demonstration Image 1 ---\n",
            "Sample True Label: Dog\n",
            "\n",
            "Zero-Shot Probabilities:\n",
            "  Horse: 0.0025\n",
            "  Cat: 0.0041\n",
            "  Dog: 0.9934\n",
            "Predicted (Zero-Shot): Dog\n",
            "\n",
            "One-Shot Probabilities:\n",
            "  Horse: 0.6153\n",
            "  Cat: 0.7313\n",
            "  Dog: 0.6746\n",
            "Predicted (One-Shot): Cat\n",
            "\n",
            "Few-Shot (3 examples) Probabilities):\n",
            "  Horse: 0.4968\n",
            "  Cat: 0.7246\n",
            "  Dog: 0.7542\n",
            "Predicted (Few-Shot): Dog\n",
            "\n",
            "--- Demonstration Image 2 ---\n",
            "Sample True Label: Dog\n",
            "\n",
            "Zero-Shot Probabilities:\n",
            "  Horse: 0.0395\n",
            "  Cat: 0.3386\n",
            "  Dog: 0.6218\n",
            "Predicted (Zero-Shot): Dog\n",
            "\n",
            "One-Shot Probabilities:\n",
            "  Horse: 0.5286\n",
            "  Cat: 0.5716\n",
            "  Dog: 0.4929\n",
            "Predicted (One-Shot): Cat\n",
            "\n",
            "Few-Shot (3 examples) Probabilities):\n",
            "  Horse: 0.4843\n",
            "  Cat: 0.5617\n",
            "  Dog: 0.4693\n",
            "Predicted (Few-Shot): Cat\n",
            "\n",
            "--- Demonstration Image 3 ---\n",
            "Sample True Label: Cat\n",
            "\n",
            "Zero-Shot Probabilities:\n",
            "  Horse: 0.0012\n",
            "  Cat: 0.9909\n",
            "  Dog: 0.0078\n",
            "Predicted (Zero-Shot): Cat\n",
            "\n",
            "One-Shot Probabilities:\n",
            "  Horse: 0.7122\n",
            "  Cat: 1.0000\n",
            "  Dog: 0.7898\n",
            "Predicted (One-Shot): Cat\n",
            "\n",
            "Few-Shot (3 examples) Probabilities):\n",
            "  Horse: 0.5511\n",
            "  Cat: 0.9518\n",
            "  Dog: 0.7231\n",
            "Predicted (Few-Shot): Cat\n",
            "\n",
            "--- Demonstration Image 4 ---\n",
            "Sample True Label: Horse\n",
            "\n",
            "Zero-Shot Probabilities:\n",
            "  Horse: 0.9996\n",
            "  Cat: 0.0001\n",
            "  Dog: 0.0003\n",
            "Predicted (Zero-Shot): Horse\n",
            "\n",
            "One-Shot Probabilities:\n",
            "  Horse: 0.8248\n",
            "  Cat: 0.6432\n",
            "  Dog: 0.6430\n",
            "Predicted (One-Shot): Horse\n",
            "\n",
            "Few-Shot (3 examples) Probabilities):\n",
            "  Horse: 0.6703\n",
            "  Cat: 0.6234\n",
            "  Dog: 0.5936\n",
            "Predicted (Few-Shot): Horse\n",
            "\n",
            "--- Demonstration Image 5 ---\n",
            "Sample True Label: Dog\n",
            "\n",
            "Zero-Shot Probabilities:\n",
            "  Horse: 0.0099\n",
            "  Cat: 0.0063\n",
            "  Dog: 0.9838\n",
            "Predicted (Zero-Shot): Dog\n",
            "\n",
            "One-Shot Probabilities:\n",
            "  Horse: 0.6292\n",
            "  Cat: 0.7788\n",
            "  Dog: 0.7570\n",
            "Predicted (One-Shot): Cat\n",
            "\n",
            "Few-Shot (3 examples) Probabilities):\n",
            "  Horse: 0.5262\n",
            "  Cat: 0.7378\n",
            "  Dog: 0.6704\n",
            "Predicted (Few-Shot): Cat\n",
            "\n",
            "--- Overall Accuracy on Demonstration 5 Images ---\n",
            "Zero-Shot Accuracy: 1.0000\n",
            "One-Shot Accuracy: 0.4000\n",
            "Few-Shot (3 examples) Accuracy: 0.6000\n"
          ]
        }
      ]
    }
  ]
}